{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c39f4f2-8d86-4381-a941-c91db6e4ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d454a879-340a-4613-aa9b-8c6e4a8a99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c0cda1-2963-48ec-b6a0-a4696880b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env, spaces, register, make\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0c57e41b-4d4c-4d09-80ea-616e2a431f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RME_file import RandomMazeEnvironment as RM_env\n",
    "from RME_file import Possibilities as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6621fa91-0121-4753-bc73-3b1926b2ff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Desktop\\CS780_Assignment1\\cs780_env\\lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment RME-mark0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "register(id = 'RME-mark0', entry_point=RM_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c75a7051-2249-427e-96b9-4135d1c7c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make('RME-mark0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c8fd28d-217c-4d1c-a3b0-cbdfdbd776e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 10 ** -10\n",
    "gamma = 0.99\n",
    "totalStates = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee84488-cc81-4d37-97d0-b7672f694723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_state_array = [x for x in range(12) if x != 5]\n",
    "real_state_array\n",
    "# since my env has no possibility of state 5 defined, \n",
    "# I make a range of 0 till 11 excluding 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa92fa18-9fe1-4817-af3c-5a41c283d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming a general policy, i.e. for each state, their is a differnt probability of taking any action out of the action space, and not that for every state, \n",
    "# there is the same probability of taking an action.\n",
    "# example\n",
    "# This is taken: policy[state = 1] = [0.1, 0.1, 0.1, 0.7] and also policy[state = 2] = [0.2, 0.3,0.0,0.5]\n",
    "# This could be the case but NOT always true: policy[state = 1] = policy[state = 2] = [0.1, 0.1, 0.1, 0.7] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f599d-38ad-490f-b8ff-0482f9f24ffb",
   "metadata": {},
   "source": [
    "# Subproblem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19143679-171a-4eb3-9c1a-004cd39a5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyEvaluation(policy, P, gamma, theta):\n",
    "    v_old = np.zeros(totalStates) # value of state 5 will never change from 0\n",
    "    \n",
    "    while True:\n",
    "        v_new = np.zeros(totalStates) # value of state 5 will never change from 0\n",
    "        for s in real_state_array:\n",
    "            for a in range(4):\n",
    "                temp = 0\n",
    "                for prob, s_next, reward, termination in P[s][a]:\n",
    "                    temp += prob*(reward + gamma*v_old[s_next])\n",
    "                if policy[s] == a:\n",
    "                    v_new[s] += 1*temp\n",
    "                else:\n",
    "                    v_new[s] += 0*temp\n",
    "        if abs(np.amax(v_new)-np.amax(v_old)) < theta:\n",
    "            break\n",
    "        else:\n",
    "            v_old = v_new\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e315e36-cf0c-4f98-a258-b2c90a4f67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyImprovement(v, P, gamma):\n",
    "    Q = np.zeros((totalStates, 4)) #total 4 actions in RME\n",
    "    policy = np.zeros(totalStates)\n",
    "    \n",
    "    for s in real_state_array:\n",
    "        for a in range(4):\n",
    "            for prob, s_next, reward, termination in P[s][a]:\n",
    "                Q[s][a] += prob*(reward + gamma* v[s_next])\n",
    "                \n",
    "    for s in real_state_array: #even if we include terminal state while updating policy,no diff as reward is 0 after they reach terminal state\n",
    "        policy[s] = np.argmax(Q[s]) #best action for state s\n",
    "    \n",
    "    #return best action for all states\n",
    "    return policy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffa5a7af-855c-40b8-81b5-d9016225f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyIteration(P, gamma, theta, initial_policy):\n",
    "    # choosing a policy to always take 0 action, meaning always go up, as initial random policy\n",
    "    policy = initial_policy\n",
    "    \n",
    "    no_iterations = 0\n",
    "    while True:\n",
    "        no_iterations += 1\n",
    "        policy_old = policy\n",
    "        v = PolicyEvaluation(policy, P, gamma, theta)\n",
    "        policy = PolicyImprovement(v, P, gamma)        \n",
    "        if np.array_equal(policy_old, policy):\n",
    "            break\n",
    "    \n",
    "    return v, policy, no_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7fc061d-af59-4300-9385-e9d2c1da502e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_policy = np.array([1,2,3,0,1,2,3,0,1,2,3,0])\n",
    "initial_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ae31931-3908-4066-9474-cb06dc182282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration:\n",
      "Initial Policy: [1 2 3 0 1 2 3 0 1 2 3 0]\n",
      "Optimal Policy: [1. 1. 1. 0. 0. 0. 0. 0. 0. 3. 0. 3.]\n",
      "Number of iterations: 3\n"
     ]
    }
   ],
   "source": [
    "v, policy, no_iterations = PolicyIteration(P, gamma, theta, initial_policy)\n",
    "np.random.seed(9569)\n",
    "print('Policy Iteration:')\n",
    "print(f'Initial Policy: {initial_policy}\\nOptimal Policy: {policy}\\nNumber of iterations: {no_iterations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0b32d-9ac3-4b3c-9619-42b812446c99",
   "metadata": {},
   "source": [
    "# SubProblem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a521970-6851-4ee7-8b2d-018ac3972b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(P,gamma,theta, initial_policy):\n",
    "    policy = initial_policy\n",
    "    v = np.zeros(totalStates)\n",
    "    iterations=0\n",
    "    \n",
    "    while True:\n",
    "        iterations += 1\n",
    "        Q=np.zeros((totalStates, 4))\n",
    "        for s in real_state_array:\n",
    "            for a in range(4):\n",
    "                for prob, s_next, reward, termination in P[s][a]:\n",
    "                    Q[s][a] += prob*(reward + gamma* v[s_next])\n",
    "                    \n",
    "        breaking_point = 1\n",
    "        \n",
    "        # if max value is smaller that theta, all the values must be smaller than theta,\n",
    "        # thus if any value is less than theta, we give break\n",
    "        for s in real_state_array:\n",
    "            if abs(v[s]-np.amax(Q[s])) >= theta:\n",
    "                breaking_point =  0\n",
    "                break\n",
    "                \n",
    "        if breaking_point == 1:\n",
    "            break\n",
    "            \n",
    "        for s in real_state_array:\n",
    "            v[s]=np.amax(Q[s])\n",
    "        \n",
    "        for s in real_state_array:\n",
    "            policy[s]=np.argmax(Q[s])\n",
    "            \n",
    "    return v, policy, iterations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a96ac128-8be1-40ab-83b9-550ba09e5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_policy = np.array([1,2,3,0,1,2,3,0,1,2,3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "998ca9bd-cf11-4736-88da-c5c1eba43ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration:\n",
      "Initial Policy: [1 2 3 0 1 2 3 0 1 2 3 0]\n",
      "Optimal Policy: [1 1 1 0 0 2 0 0 0 3 0 3]\n",
      "Number of iterations: 38\n"
     ]
    }
   ],
   "source": [
    "v, optimal_policy, no_iterations = ValueIteration(P, gamma, theta, np.array([1,2,3,0,1,2,3,0,1,2,3,0]))\n",
    "np.random.seed(9569)\n",
    "print('Value Iteration:')\n",
    "print(f'Initial Policy: {initial_policy}\\nOptimal Policy: {optimal_policy}\\nNumber of iterations: {no_iterations}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs780_env",
   "language": "python",
   "name": "cs780_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
